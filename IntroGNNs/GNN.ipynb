{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb216191-0469-4deb-8dbf-dedec545fae7",
   "metadata": {},
   "source": [
    "Volvemos a cargar el dataset y una serie de otras cosas. \n",
    "\n",
    "Basado en este excelente [tutorial](https://keras.io/examples/graph/gnn_citations/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f636c29a-9661-40e2-9a8a-85d549707f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "citations = pd.read_csv(\"cora/cora.cites\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"target\", \"source\"],\n",
    ")\n",
    "\n",
    "column_names = [\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"]\n",
    "papers = pd.read_csv(\"cora/cora.content\", sep=\"\\t\", header=None, names=column_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7d5fb04-2e7d-4155-b443-e96199d4afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (1367, 1435)\n",
      "Test data shape: (1341, 1435)\n"
     ]
    }
   ],
   "source": [
    "class_values = sorted(papers[\"subject\"].unique())\n",
    "class_idx = {name: id for id, name in enumerate(class_values)}\n",
    "paper_idx = {name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))}\n",
    "\n",
    "papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n",
    "citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n",
    "citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n",
    "papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])\n",
    "\n",
    "train_data, test_data = [], []\n",
    "\n",
    "for _, group_data in papers.groupby(\"subject\"):\n",
    "    # Select around 50% of the dataset for training.\n",
    "    random_selection = np.random.rand(len(group_data.index)) <= 0.5\n",
    "    train_data.append(group_data[random_selection])\n",
    "    test_data.append(group_data[~random_selection])\n",
    "\n",
    "train_data = pd.concat(train_data).sample(frac=1)\n",
    "test_data = pd.concat(test_data).sample(frac=1)\n",
    "\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ada8ae04-eef0-4e7b-a636-9cdda22267bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_MLP(hidden_layers, dropout_rate, name=None):\n",
    "    model = nn.Sequential()\n",
    "    num_layer = 0\n",
    "\n",
    "    for layer in hidden_layers:\n",
    "        num_layer +=1\n",
    "        neurons_in = layer[0]\n",
    "        neurons_out = layer[1]\n",
    "        \n",
    "        model.add_module(\"batchnorm\"+str(num_layer), nn.BatchNorm1d(neurons_in))\n",
    "        model.add_module(\"dropout\"+str(num_layer), nn.Dropout(dropout_rate))\n",
    "        model.add_module(\"dense\"+str(num_layer), nn.Linear(neurons_in, neurons_out))\n",
    "        model.add_module(\"activacion\"+str(num_layer), nn.GELU())\n",
    "    \n",
    "    return(model)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a258a6-e44f-4130-b52a-1ca113f1e72f",
   "metadata": {},
   "source": [
    "## Manejo de datos específico para nuestras GNNs.\n",
    "\n",
    "Lo primero es que ahora las GNNs van a funcionar en base a las conexiones entro los papers (además de los features obviamente). La GNN se compila con la info del grado, por lo que el x_train y x_test solo deben tener los id de los nodos relevantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31f1e779-856d-4c65-92c4-4335ffb33778",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(set(papers.columns) - {\"paper_id\", \"subject\"})\n",
    "num_features = len(feature_names)\n",
    "num_classes = len(class_idx)\n",
    "\n",
    "# Create train and test features as a numpy array.\n",
    "x_train = train_data[\"paper_id\"].to_numpy()\n",
    "x_test = test_data[\"paper_id\"].to_numpy()\n",
    "# Create train and test targets as a numpy array.\n",
    "y_train = train_data[\"subject\"]\n",
    "y_test = test_data[\"subject\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f85017e-db59-4194-a5a4-a93dbcdc03ea",
   "metadata": {},
   "source": [
    "El segundo paso es crear una matriz de adyacencia en formato numpy, que es lo que vamos a necesitar para pasarselo a torch. Por razones de formato, es mejor usar una representación esparsa, en forma de lista de pares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "539b28c6-b2ad-439b-a373-086e40f34fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges shape: (2, 5429)\n",
      "Nodes shape: torch.Size([2708, 1433])\n"
     ]
    }
   ],
   "source": [
    "#Matriz en forma de lista de pares\n",
    "edges = citations[[\"source\", \"target\"]].to_numpy().T\n",
    "\n",
    "#Codigo para agregar peso a cada arista, por ahora son puros 1s, todas valen lo mismo. \n",
    "edge_weights = torch.ones(edges.shape[1])\n",
    "\n",
    "# Crear (en formato torch) los features para cada nodo.\n",
    "node_features = papers.sort_values(\"paper_id\")[feature_names].to_numpy()\n",
    "node_features = torch.tensor(node_features, dtype=torch.float32)\n",
    "\n",
    "# el grafo es la union de estas tres cosas\n",
    "graph_info = (node_features, edges, edge_weights)\n",
    "\n",
    "print(\"Edges shape:\", edges.shape)\n",
    "print(\"Nodes shape:\", node_features.shape)\n",
    "\n",
    "\n",
    "### Esto es muy importante. \n",
    "### El primer vector es la lista de los indices de los nodos source de edges, \n",
    "### El segundo vector es la lista de los indices de los nodos target\n",
    "\n",
    "node_indices, neighbour_indices = edges[0], edges[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6442b37-0e94-46ed-bf6b-59498ab88d50",
   "metadata": {},
   "source": [
    "### Un modelo para una capa de la GNN\n",
    "\n",
    "Esta es la capa que va a hacer los pasos de agregación y update.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607eecd-ec75-41ac-b128-9d6f6c1d4b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, input_features, capas_internas=[32, 32], dropout_rate=0.2, normalize=False):\n",
    "        super(GNNLayer, self).__init__()\n",
    "\n",
    "        #Hay dos redes neuronales involucradas en una capa de GNN: la primera activación de los mensajes, \n",
    "        #    y el manejo del update. \n",
    "        \n",
    "        self.preprocessor = create_MLP([[input_features,hidden_layer_neurons],[hidden_layer_neurons,hidden_layer_neurons]], dropout_rate)\n",
    "        self.updater = create_MLP([[hidden_layer_neurons,hidden_layer_neurons],[hidden_layer_neurons,hidden_layer_neurons]], dropout_rate)\n",
    "\n",
    "    def prepare(self, node_representations, weights=None):\n",
    "        \n",
    "        #Esta funcion pasa los mensajes por una red neuronal simple, y aplica los pesos (si hay)\n",
    "        \n",
    "        messages = self.preprocessor(node_representations)\n",
    "        if weights is not None:\n",
    "            messages = messages * weights.unsqueeze(-1)\n",
    "        return messages\n",
    "\n",
    "    def aggregate(self, node_indices, neighbour_messages, node_representations):\n",
    "        # Esta funcion agrega los mensajes de cada nodo, en forma de suma. \n",
    "        # recibo un vector node_indices, que es de largo [num_edges] y me dice los nodos origen de cada arista\n",
    "        # matriz neighbour_messages es de forma [num_edges, (neuronas_internas)], osea [num_edges, 32] en este codigo\n",
    "        # esta matriz tiene el mensaje de cada nodo que participa en la arista como nodo destino\n",
    "        # la matriz node_repesentations es de la forma [num_nodes, representation_dim], contiene información de \n",
    "        # los nodos del grafo. \n",
    "\n",
    "        num_nodes = node_repesentations.shape[0]\n",
    "\n",
    "        #### JUAN: POR PONER\n",
    "        \n",
    "        aggregated_message = torch.zeros((num_nodes, neighbour_messages.shape[1])).to(neighbour_messages.device)\n",
    "        aggregated_message = aggregated_message.scatter_add_(0, node_indices.unsqueeze(-1).expand_as(neighbour_messages), neighbour_messages)\n",
    "\n",
    "    def update(self, node_representations, aggregated_messages):\n",
    "        # Para combinar los mensajes con los features de cada nodo, concatenamos. \n",
    "        # Notar que a este punto tanto node_repesentations como aggregated_messages tienen forma \n",
    "        # [num_nodes, representation_dim]. Cat los concatena. \n",
    "\n",
    "        h = torch.cat([node_representations, aggregated_messages], dim=1)\n",
    "\n",
    "        # Y aplicamos unas capas no-lineales\n",
    "        \n",
    "        node_embeddings = self.updater(h)\n",
    "\n",
    "        return node_embeddings\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## Procesa los inputs para crear los embeddings. Siempre tenemos información de todo el grafo, \n",
    "        ## y operamos sobre todos los nodos en node_representations\n",
    "        \n",
    "        node_representations, edges, edge_weights = inputs\n",
    "        node_indices, neighbour_indices = edges\n",
    "\n",
    "        # Lo primero es una lista de vectores en donde tomo cada id en neighbour_indices \n",
    "        # y lo reemplazo por la representación de ese id. \n",
    "        # El resultado es una lista que contiene, para cada arista, la representación del target de esa arista\n",
    "        neighbour_representations = node_representations[neighbour_indices]\n",
    "\n",
    "        # Procesamos estos mensajes (posiblemente incluyendo pesos en aristas)\n",
    "        neighbour_messages = self.prepare(neighbour_representations, edge_weights)\n",
    "        \n",
    "        # Los agregamos\n",
    "        aggregated_messages = self.aggregate(node_indices, neighbour_messages, node_representations)\n",
    "        \n",
    "        # Y finalmente el update\n",
    "        return self.update(node_representations, aggregated_messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd5ae4c-64c2-446f-831d-0bfc5cab40b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ahora juntamos varias capas! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb81e0d-0940-4594-bebc-674dedf8cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNClassifier(nn.Module):\n",
    "    def __init__(self, graph_info, num_classes, input_features, hidden_layer_neurons=32, dropout_rate=0.2, normalize=True):\n",
    "        super(GNNClassifier, self).__init__()\n",
    "\n",
    "        #LA GNN maneja información de todo el grafo, independiente del batch que procese. \n",
    "\n",
    "        node_features, edges, edge_weights = graph_info\n",
    "        self.node_features = node_features\n",
    "        self.edges = edges\n",
    "        self.edge_weights = edge_weights\n",
    "        \n",
    "        #normalizar\n",
    "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
    "\n",
    "        #Las layers básicas: una capa para preprocesar todo \n",
    "        self.preprocessor = create_MLP([[self.node_features,hidden_layer_neurons],[hidden_layer_neurons,hidden_layer_neurons]], dropout_rate)\n",
    "\n",
    "        # dos capas de paso de mensjaes \n",
    "        \n",
    "        self.layer1 = GNNLayer(hidden_layer_neurons, hidden_layer_neurons, dropout_rate=0.2, normalize=False)\n",
    "        self.layer2 = GNNLayer(hidden_layer_neurons, hidden_layer_neurons, dropout_rate=0.2, normalize=False)\n",
    "\n",
    "        # Un capa final.\n",
    "        \n",
    "        self.postprocesado = create_MLP([[hidden_layer_neurons,hidden_layer_neurons],[hidden_layer_neurons,hidden_layer_neurons]], dropout_rate)\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_layer_neurons, num_classes)\n",
    "\n",
    "    def forward(self, batch_indices):\n",
    "        \n",
    "        #### Feature preprocessing layer to reduce dimensionality\n",
    "        nodos_preprocesados = self.preprocessor(self.node_features)\n",
    "\n",
    "        #### These preprocessed nodes go through capa1, which aggregates messages from their neighbors.\n",
    "        paso_mens1 = self.capa1((nodos_preprocesados, self.edges, self.edge_weights))\n",
    "\n",
    "        skip1 = nodos_preprocesados + paso_mens1\n",
    "\n",
    "        paso_mens2 = self.capa2((skip1, self.edges, self.edge_weights))\n",
    "\n",
    "        skip2 = paso_mens2 + skip1\n",
    "\n",
    "        ##### Postprocessing to get to the node category\n",
    "        postprocesado = self.postprocess(skip2)\n",
    "\n",
    "        ##### Put embeddings back in the order required by the batch\n",
    "        node_embeddings = postprocesado[batch_indices]\n",
    "\n",
    "        # Readout to get to the categories\n",
    "        return self.clas(node_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
