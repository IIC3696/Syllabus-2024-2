{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jAVmYfzrXyB4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Leer archivos CORA\n",
        "citations = pd.read_csv(\"cora/cora.cites\", sep=\"\\t\", header=None, names=[\"target\", \"source\"])\n",
        "\n",
        "column_names = [\"paper_id\"] + [f\"term_{idx}\" for idx in range(1433)] + [\"subject\"]\n",
        "papers = pd.read_csv(\"cora/cora.content\", sep=\"\\t\", header=None, names=column_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar set de train y test\n",
        "class_values = sorted(papers[\"subject\"].unique())\n",
        "class_idx = {name: id for id, name in enumerate(class_values)}\n",
        "paper_idx = {name: idx for idx, name in enumerate(sorted(papers[\"paper_id\"].unique()))}\n",
        "\n",
        "papers[\"paper_id\"] = papers[\"paper_id\"].apply(lambda name: paper_idx[name])\n",
        "citations[\"source\"] = citations[\"source\"].apply(lambda name: paper_idx[name])\n",
        "citations[\"target\"] = citations[\"target\"].apply(lambda name: paper_idx[name])\n",
        "papers[\"subject\"] = papers[\"subject\"].apply(lambda value: class_idx[value])\n",
        "\n",
        "train_data, test_data = [], []\n",
        "\n",
        "for _, group_data in papers.groupby(\"subject\"):\n",
        "    # Select around 50% of the dataset for training.\n",
        "    random_selection = np.random.rand(len(group_data.index)) <= 0.5\n",
        "    train_data.append(group_data[random_selection])\n",
        "    test_data.append(group_data[~random_selection])\n",
        "\n",
        "train_data = pd.concat(train_data).sample(frac=1)\n",
        "test_data = pd.concat(test_data).sample(frac=1)\n",
        "\n",
        "print(\"Train data shape:\", train_data.shape)\n",
        "print(\"Test data shape:\", test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exGoCt7YYkGb",
        "outputId": "1aedd2df-b780-4b10-c1b4-4e78c8a383d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape: (1362, 1435)\n",
            "Test data shape: (1346, 1435)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "4TiqYVViZq4-",
        "outputId": "ff25f56c-af56-4c90-d03a-dae2053fee64"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      paper_id  term_0  term_1  term_2  term_3  term_4  term_5  term_6  \\\n",
              "1232      1583       0       0       0       0       0       0       0   \n",
              "1         1911       0       0       0       0       0       0       0   \n",
              "1037      1012       0       0       0       0       0       0       0   \n",
              "1009      2675       0       0       0       0       0       0       0   \n",
              "274        851       0       0       0       0       0       0       0   \n",
              "\n",
              "      term_7  term_8  ...  term_1424  term_1425  term_1426  term_1427  \\\n",
              "1232       0       0  ...          0          0          0          0   \n",
              "1          0       0  ...          0          1          0          0   \n",
              "1037       0       0  ...          0          0          0          0   \n",
              "1009       0       0  ...          0          0          0          0   \n",
              "274        0       0  ...          0          0          0          1   \n",
              "\n",
              "      term_1428  term_1429  term_1430  term_1431  term_1432  subject  \n",
              "1232          0          0          0          0          0        1  \n",
              "1             0          0          0          0          0        5  \n",
              "1037          0          0          0          0          0        2  \n",
              "1009          0          0          0          0          0        1  \n",
              "274           0          0          0          0          0        4  \n",
              "\n",
              "[5 rows x 1435 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b67c7bf3-3141-468a-a572-145ce5d50f92\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_id</th>\n",
              "      <th>term_0</th>\n",
              "      <th>term_1</th>\n",
              "      <th>term_2</th>\n",
              "      <th>term_3</th>\n",
              "      <th>term_4</th>\n",
              "      <th>term_5</th>\n",
              "      <th>term_6</th>\n",
              "      <th>term_7</th>\n",
              "      <th>term_8</th>\n",
              "      <th>...</th>\n",
              "      <th>term_1424</th>\n",
              "      <th>term_1425</th>\n",
              "      <th>term_1426</th>\n",
              "      <th>term_1427</th>\n",
              "      <th>term_1428</th>\n",
              "      <th>term_1429</th>\n",
              "      <th>term_1430</th>\n",
              "      <th>term_1431</th>\n",
              "      <th>term_1432</th>\n",
              "      <th>subject</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1232</th>\n",
              "      <td>1583</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1911</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1037</th>\n",
              "      <td>1012</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1009</th>\n",
              "      <td>2675</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>851</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1435 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b67c7bf3-3141-468a-a572-145ce5d50f92')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b67c7bf3-3141-468a-a572-145ce5d50f92 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b67c7bf3-3141-468a-a572-145ce5d50f92');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b6b1b170-466c-4b31-a26e-05fdac27631d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b6b1b170-466c-4b31-a26e-05fdac27631d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b6b1b170-466c-4b31-a26e-05fdac27631d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_data"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manejo de datos específico para nuestras GNNs.\n",
        "Lo primero es que ahora las GNNs van a funcionar en base a las conexiones entro los papers (además de los features obviamente). La GNN se compila con la info del grado, por lo que el x_train y x_test solo deben tener los id de los nodos relevantes."
      ],
      "metadata": {
        "id": "K9BV8jnuaA6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = list(set(papers.columns) - {\"paper_id\", \"subject\"})\n",
        "num_features = len(feature_names)\n",
        "num_classes = len(class_idx)\n",
        "\n",
        "# Crear train y test features (X).\n",
        "x_train = train_data[\"paper_id\"].to_numpy()\n",
        "x_test = test_data[\"paper_id\"].to_numpy()\n",
        "# Crear train y test targets (y).\n",
        "y_train = train_data[\"subject\"]\n",
        "y_test = test_data[\"subject\"]"
      ],
      "metadata": {
        "id": "XlOF5OZYaCKy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Matriz en forma de lista de pares\n",
        "edges = citations[[\"source\", \"target\"]].to_numpy().T\n",
        "\n",
        "# Agregar peso a cada arista, por ahora son todos 1s, todas valen lo mismo.\n",
        "edge_weights = torch.ones(edges.shape[1])\n",
        "\n",
        "# Crear los features para cada nodo.\n",
        "node_features = torch.tensor(\n",
        "    papers.sort_values(\"paper_id\")[feature_names].to_numpy(), dtype=torch.float32\n",
        ")\n",
        "\n",
        "# El grafo es la unión de estas tres cosas\n",
        "graph_info = (node_features, edges, edge_weights)\n",
        "\n",
        "print(\"Edges shape:\", edges.shape)\n",
        "print(\"Nodes shape:\", node_features.shape)\n",
        "\n",
        "# El primer vector es la lista de los índices de los nodos source de edges,\n",
        "# El segundo vector es la lista de los índices de los nodos target\n",
        "node_indices, neighbour_indices = edges[0], edges[1]\n",
        "\n",
        "# Imprimir los índices de los nodos y sus vecinos\n",
        "print(\"Node indices:\", node_indices)\n",
        "print(\"Neighbour indices:\", neighbour_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3vR2qk5aGZ6",
        "outputId": "ccc34c2c-57d7-4747-ce90-96c0fea4beda"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edges shape: (2, 5429)\n",
            "Nodes shape: torch.Size([2708, 1433])\n",
            "Node indices: [  21  905  906 ... 2586 1874 2707]\n",
            "Neighbour indices: [   0    0    0 ... 1874 1876 1897]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Un modelo para una capa de la GNN\n",
        "Esta es la capa que va a hacer los pasos de agregación y update. Se van a definir estas operaciones, y las necesarias para que funcione el modelo"
      ],
      "metadata": {
        "id": "jakpS0YRbKY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_MLP(layers_dims, dropout_rate):\n",
        "    layers = []\n",
        "    for i in range(len(layers_dims) - 1):\n",
        "        layers.append(nn.Linear(layers_dims[i], layers_dims[i+1]))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Dropout(dropout_rate))\n",
        "    return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "TWJfyfDkbIFA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout_rate=0.2):\n",
        "        super(GNNLayer, self).__init__()\n",
        "\n",
        "        # Preprocesador que transforma las representaciones de nodos antes de la agregación\n",
        "        self.preprocesador = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "        # Updater que combina las representaciones de nodos con los mensajes agregados\n",
        "        self.updater = nn.Sequential(\n",
        "            nn.Linear(input_dim + output_dim, output_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "    def prepare(self, node_representations, weights=None):\n",
        "        # Preprocesar las representaciones de los vecinos para la agregación\n",
        "        messages = self.preprocesador(node_representations)\n",
        "        if weights is not None:\n",
        "            messages = messages * weights.unsqueeze(-1)\n",
        "        return messages\n",
        "\n",
        "    def aggregate(self, node_indices, neighbour_messages, node_representations):\n",
        "        node_indices = torch.tensor(node_indices, dtype=torch.long)\n",
        "        num_nodes = node_representations.size(0)\n",
        "        # Tensor para almacenar los mensajes agregados\n",
        "        aggregated_message = torch.zeros((num_nodes, neighbour_messages.size(1)), dtype=neighbour_messages.dtype)\n",
        "        # Agregar los mensajes de los vecinos a los nodos correspondientes\n",
        "        aggregated_message.index_add_(0, node_indices, neighbour_messages)\n",
        "        return aggregated_message\n",
        "\n",
        "    def update(self, node_representations, aggregated_messages):\n",
        "        # Concatenar las representaciones de los nodos con los mensajes agregados\n",
        "        h = torch.cat([node_representations, aggregated_messages], dim=1)\n",
        "        # Actualizar las representaciones de los nodos utilizando el Updater\n",
        "        node_embeddings = self.updater(h)\n",
        "        return node_embeddings\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        node_representations, edges, edge_weights = inputs\n",
        "        node_indices, neighbour_indices = edges\n",
        "        node_indices = torch.tensor(node_indices, dtype=torch.long)\n",
        "        neighbour_indices = torch.tensor(neighbour_indices, dtype=torch.long)\n",
        "\n",
        "        # Obtener las representaciones de los vecinos\n",
        "        neighbour_representations = node_representations[neighbour_indices]\n",
        "        # Preprocesar los mensajes de los vecinos\n",
        "        neighbour_messages = self.prepare(neighbour_representations, edge_weights)\n",
        "        # Agregar los mensajes de los vecinos\n",
        "        aggregated_messages = self.aggregate(node_indices, neighbour_messages, node_representations)\n",
        "        # Actualizar las representaciones de los nodos\n",
        "        return self.update(node_representations, aggregated_messages)\n"
      ],
      "metadata": {
        "id": "uYgeP87bbj1X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Juntando todo en un clasificador\n",
        "Ahora si, definimos un modelo igual que la vez anterior, solo que ahora usa nuestra capa!\n",
        "\n"
      ],
      "metadata": {
        "id": "jNJ3Lz2qdh6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNBasica(nn.Module):\n",
        "    def __init__(self, graph_info, num_classes, hidden_dims=[32, 32], dropout_rate=0.2):\n",
        "        super(GNNBasica, self).__init__()\n",
        "        # Desempaquetar la información del grafo\n",
        "        node_features, edges, edge_weights = graph_info\n",
        "        self.node_features = node_features\n",
        "        self.edges = edges\n",
        "        self.edge_weights = edge_weights / edge_weights.sum()\n",
        "\n",
        "        input_dim = node_features.size(1)\n",
        "        # Crear MLP para preprocesar las características de los nodos\n",
        "        self.preprocesar = create_MLP([input_dim] + hidden_dims, dropout_rate)\n",
        "\n",
        "        # Crear capas de la red neuronal gráfica (GNN)\n",
        "        self.capa1 = GNNLayer(hidden_dims[-1], hidden_dims[-1], dropout_rate)\n",
        "        self.capa2 = GNNLayer(hidden_dims[-1], hidden_dims[-1], dropout_rate)\n",
        "\n",
        "        # Crear MLP para el postprocesamiento de los nodos\n",
        "        self.postprocess = create_MLP(hidden_dims, dropout_rate)\n",
        "        self.clas = nn.Linear(hidden_dims[-1], num_classes)\n",
        "\n",
        "    def forward(self, node_indices):\n",
        "        nodos_preprocesados = self.preprocesar(self.node_features)\n",
        "        # Pasar las características preprocesadas a la primera capa GNN\n",
        "        paso_mens1 = self.capa1((nodos_preprocesados, self.edges, self.edge_weights))\n",
        "        skip1 = nodos_preprocesados + paso_mens1 # Residual connection\n",
        "\n",
        "        # Pasar a la segunda capa GNN\n",
        "        paso_mens2 = self.capa2((skip1, self.edges, self.edge_weights))\n",
        "        skip2 = paso_mens2 + skip1 # Residual connection\n",
        "        postprocesado = self.postprocess(skip2)\n",
        "\n",
        "        # Obtener las representaciones para los índices dados\n",
        "        node_embeddings = postprocesado[node_indices]\n",
        "        return self.clas(node_embeddings)"
      ],
      "metadata": {
        "id": "j99yvL0MdlyE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializar modelo\n",
        "gnn_model = GNNBasica(\n",
        "    graph_info=graph_info,\n",
        "    num_classes=num_classes,\n",
        "    hidden_dims=[32, 32],\n",
        "    dropout_rate=0.2\n",
        ")\n",
        "\n",
        "print(gnn_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T094z2g4d9sY",
        "outputId": "9e8f8ec6-f480-4276-8668-fa37f875af6b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GNNBasica(\n",
            "  (preprocesar): Sequential(\n",
            "    (0): Linear(in_features=1433, out_features=32, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (capa1): GNNLayer(\n",
            "    (preprocesador): Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (updater): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (capa2): GNNLayer(\n",
            "    (preprocesador): Sequential(\n",
            "      (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "    (updater): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Dropout(p=0.2, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (postprocess): Sequential(\n",
            "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (clas): Linear(in_features=32, out_features=7, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.01)\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=50, delta=0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.counter = 0\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, val_acc, model):\n",
        "        score = val_acc\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                model.load_state_dict(self.best_weights)\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        self.best_weights = model.state_dict()\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
        "\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
        "\n",
        "# Dividir los datos de entrenamiento en entrenamiento y validación (Keras lo separa internamente con \"validation_split\")\n",
        "x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train_tensor, y_train_tensor, test_size=0.15)\n",
        "x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(x_train_tensor, y_train_tensor, test_size=0.15)\n",
        "\n",
        "epochs = 300\n",
        "early_stopping = EarlyStopping(patience=50)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    gnn_model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = gnn_model(x_train_split)\n",
        "    loss = criterion(outputs, y_train_split)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Validación\n",
        "    gnn_model.eval()\n",
        "    val_acc = 0\n",
        "    with torch.no_grad():\n",
        "        outputs = gnn_model(x_val_split)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct = (predicted == y_val_split).sum().item()\n",
        "        total = y_val_split.size(0)\n",
        "        val_acc = correct / total\n",
        "\n",
        "    print(f\"Epoch {epoch+1:03d}, Loss: {loss.item():.5f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    early_stopping(val_acc, gnn_model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping\")\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3xdEqbWeJIq",
        "outputId": "0a14f086-0421-4c0b-966e-90e8ba35c968"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-54102013a12b>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  node_indices = torch.tensor(node_indices, dtype=torch.long)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001, Loss: 1.93900, Val Acc: 0.2829\n",
            "Epoch 002, Loss: 1.91375, Val Acc: 0.2829\n",
            "Epoch 003, Loss: 1.89442, Val Acc: 0.2829\n",
            "Epoch 004, Loss: 1.87075, Val Acc: 0.2829\n",
            "Epoch 005, Loss: 1.82836, Val Acc: 0.2829\n",
            "Epoch 006, Loss: 1.77332, Val Acc: 0.2829\n",
            "Epoch 007, Loss: 1.72553, Val Acc: 0.2829\n",
            "Epoch 008, Loss: 1.65797, Val Acc: 0.2878\n",
            "Epoch 009, Loss: 1.56678, Val Acc: 0.3512\n",
            "Epoch 010, Loss: 1.48786, Val Acc: 0.3707\n",
            "Epoch 011, Loss: 1.38816, Val Acc: 0.4049\n",
            "Epoch 012, Loss: 1.29528, Val Acc: 0.4244\n",
            "Epoch 013, Loss: 1.20073, Val Acc: 0.4439\n",
            "Epoch 014, Loss: 1.11611, Val Acc: 0.4732\n",
            "Epoch 015, Loss: 1.06174, Val Acc: 0.4780\n",
            "Epoch 016, Loss: 0.96783, Val Acc: 0.5171\n",
            "Epoch 017, Loss: 0.90052, Val Acc: 0.5171\n",
            "Epoch 018, Loss: 0.81517, Val Acc: 0.5512\n",
            "Epoch 019, Loss: 0.74735, Val Acc: 0.5854\n",
            "Epoch 020, Loss: 0.67847, Val Acc: 0.6049\n",
            "Epoch 021, Loss: 0.63345, Val Acc: 0.6049\n",
            "Epoch 022, Loss: 0.53654, Val Acc: 0.6000\n",
            "Epoch 023, Loss: 0.50542, Val Acc: 0.5902\n",
            "Epoch 024, Loss: 0.46925, Val Acc: 0.5951\n",
            "Epoch 025, Loss: 0.41015, Val Acc: 0.6000\n",
            "Epoch 026, Loss: 0.40427, Val Acc: 0.6146\n",
            "Epoch 027, Loss: 0.35637, Val Acc: 0.6146\n",
            "Epoch 028, Loss: 0.32458, Val Acc: 0.6244\n",
            "Epoch 029, Loss: 0.30777, Val Acc: 0.6488\n",
            "Epoch 030, Loss: 0.29008, Val Acc: 0.6634\n",
            "Epoch 031, Loss: 0.24439, Val Acc: 0.6634\n",
            "Epoch 032, Loss: 0.24205, Val Acc: 0.6829\n",
            "Epoch 033, Loss: 0.26876, Val Acc: 0.6732\n",
            "Epoch 034, Loss: 0.23008, Val Acc: 0.6732\n",
            "Epoch 035, Loss: 0.17624, Val Acc: 0.6829\n",
            "Epoch 036, Loss: 0.19206, Val Acc: 0.6878\n",
            "Epoch 037, Loss: 0.17265, Val Acc: 0.6829\n",
            "Epoch 038, Loss: 0.15357, Val Acc: 0.6829\n",
            "Epoch 039, Loss: 0.14339, Val Acc: 0.6780\n",
            "Epoch 040, Loss: 0.12073, Val Acc: 0.6585\n",
            "Epoch 041, Loss: 0.14205, Val Acc: 0.6634\n",
            "Epoch 042, Loss: 0.11278, Val Acc: 0.6537\n",
            "Epoch 043, Loss: 0.09523, Val Acc: 0.6488\n",
            "Epoch 044, Loss: 0.11767, Val Acc: 0.6537\n",
            "Epoch 045, Loss: 0.09523, Val Acc: 0.6634\n",
            "Epoch 046, Loss: 0.07551, Val Acc: 0.6634\n",
            "Epoch 047, Loss: 0.11609, Val Acc: 0.6585\n",
            "Epoch 048, Loss: 0.09840, Val Acc: 0.6634\n",
            "Epoch 049, Loss: 0.10128, Val Acc: 0.6585\n",
            "Epoch 050, Loss: 0.07335, Val Acc: 0.6634\n",
            "Epoch 051, Loss: 0.07271, Val Acc: 0.6634\n",
            "Epoch 052, Loss: 0.07981, Val Acc: 0.6683\n",
            "Epoch 053, Loss: 0.07027, Val Acc: 0.6780\n",
            "Epoch 054, Loss: 0.06885, Val Acc: 0.6780\n",
            "Epoch 055, Loss: 0.07558, Val Acc: 0.6780\n",
            "Epoch 056, Loss: 0.08180, Val Acc: 0.6683\n",
            "Epoch 057, Loss: 0.06515, Val Acc: 0.6683\n",
            "Epoch 058, Loss: 0.06328, Val Acc: 0.6732\n",
            "Epoch 059, Loss: 0.05597, Val Acc: 0.6732\n",
            "Epoch 060, Loss: 0.05463, Val Acc: 0.6780\n",
            "Epoch 061, Loss: 0.03807, Val Acc: 0.6829\n",
            "Epoch 062, Loss: 0.06475, Val Acc: 0.6780\n",
            "Epoch 063, Loss: 0.05890, Val Acc: 0.6732\n",
            "Epoch 064, Loss: 0.06176, Val Acc: 0.6732\n",
            "Epoch 065, Loss: 0.06281, Val Acc: 0.6780\n",
            "Epoch 066, Loss: 0.05489, Val Acc: 0.6829\n",
            "Epoch 067, Loss: 0.04839, Val Acc: 0.6829\n",
            "Epoch 068, Loss: 0.04356, Val Acc: 0.6829\n",
            "Epoch 069, Loss: 0.03950, Val Acc: 0.6780\n",
            "Epoch 070, Loss: 0.04366, Val Acc: 0.6683\n",
            "Epoch 071, Loss: 0.06050, Val Acc: 0.6780\n",
            "Epoch 072, Loss: 0.03251, Val Acc: 0.6878\n",
            "Epoch 073, Loss: 0.04946, Val Acc: 0.6829\n",
            "Epoch 074, Loss: 0.02976, Val Acc: 0.7024\n",
            "Epoch 075, Loss: 0.02978, Val Acc: 0.7024\n",
            "Epoch 076, Loss: 0.03673, Val Acc: 0.6829\n",
            "Epoch 077, Loss: 0.02839, Val Acc: 0.6829\n",
            "Epoch 078, Loss: 0.04491, Val Acc: 0.6780\n",
            "Epoch 079, Loss: 0.03443, Val Acc: 0.6780\n",
            "Epoch 080, Loss: 0.04112, Val Acc: 0.6829\n",
            "Epoch 081, Loss: 0.03119, Val Acc: 0.6878\n",
            "Epoch 082, Loss: 0.03380, Val Acc: 0.6878\n",
            "Epoch 083, Loss: 0.04510, Val Acc: 0.6927\n",
            "Epoch 084, Loss: 0.02008, Val Acc: 0.6976\n",
            "Epoch 085, Loss: 0.04235, Val Acc: 0.6976\n",
            "Epoch 086, Loss: 0.03457, Val Acc: 0.6976\n",
            "Epoch 087, Loss: 0.02896, Val Acc: 0.6927\n",
            "Epoch 088, Loss: 0.03402, Val Acc: 0.6878\n",
            "Epoch 089, Loss: 0.03770, Val Acc: 0.6976\n",
            "Epoch 090, Loss: 0.02095, Val Acc: 0.6976\n",
            "Epoch 091, Loss: 0.04101, Val Acc: 0.7024\n",
            "Epoch 092, Loss: 0.01865, Val Acc: 0.7024\n",
            "Epoch 093, Loss: 0.03475, Val Acc: 0.7024\n",
            "Epoch 094, Loss: 0.02920, Val Acc: 0.7073\n",
            "Epoch 095, Loss: 0.05050, Val Acc: 0.6927\n",
            "Epoch 096, Loss: 0.01422, Val Acc: 0.6976\n",
            "Epoch 097, Loss: 0.02425, Val Acc: 0.6976\n",
            "Epoch 098, Loss: 0.03486, Val Acc: 0.6927\n",
            "Epoch 099, Loss: 0.01733, Val Acc: 0.6878\n",
            "Epoch 100, Loss: 0.04270, Val Acc: 0.6878\n",
            "Epoch 101, Loss: 0.02574, Val Acc: 0.6829\n",
            "Epoch 102, Loss: 0.01152, Val Acc: 0.6780\n",
            "Epoch 103, Loss: 0.02872, Val Acc: 0.6829\n",
            "Epoch 104, Loss: 0.02245, Val Acc: 0.6829\n",
            "Epoch 105, Loss: 0.01940, Val Acc: 0.6780\n",
            "Epoch 106, Loss: 0.04620, Val Acc: 0.6829\n",
            "Epoch 107, Loss: 0.01803, Val Acc: 0.6878\n",
            "Epoch 108, Loss: 0.02043, Val Acc: 0.6976\n",
            "Epoch 109, Loss: 0.02499, Val Acc: 0.6976\n",
            "Epoch 110, Loss: 0.01082, Val Acc: 0.6976\n",
            "Epoch 111, Loss: 0.02632, Val Acc: 0.7073\n",
            "Epoch 112, Loss: 0.02560, Val Acc: 0.7073\n",
            "Epoch 113, Loss: 0.02701, Val Acc: 0.7024\n",
            "Epoch 114, Loss: 0.02347, Val Acc: 0.6878\n",
            "Epoch 115, Loss: 0.02167, Val Acc: 0.6878\n",
            "Epoch 116, Loss: 0.02087, Val Acc: 0.6829\n",
            "Epoch 117, Loss: 0.02689, Val Acc: 0.6732\n",
            "Epoch 118, Loss: 0.01521, Val Acc: 0.6732\n",
            "Epoch 119, Loss: 0.01643, Val Acc: 0.6780\n",
            "Epoch 120, Loss: 0.01890, Val Acc: 0.6780\n",
            "Epoch 121, Loss: 0.01655, Val Acc: 0.6878\n",
            "Epoch 122, Loss: 0.02040, Val Acc: 0.6927\n",
            "Epoch 123, Loss: 0.03789, Val Acc: 0.6976\n",
            "Epoch 124, Loss: 0.01487, Val Acc: 0.6976\n",
            "Epoch 125, Loss: 0.01669, Val Acc: 0.6927\n",
            "Epoch 126, Loss: 0.02926, Val Acc: 0.6927\n",
            "Epoch 127, Loss: 0.01899, Val Acc: 0.6927\n",
            "Epoch 128, Loss: 0.02204, Val Acc: 0.6927\n",
            "Epoch 129, Loss: 0.00739, Val Acc: 0.6878\n",
            "Epoch 130, Loss: 0.01469, Val Acc: 0.6829\n",
            "Epoch 131, Loss: 0.01277, Val Acc: 0.6829\n",
            "Epoch 132, Loss: 0.02140, Val Acc: 0.6829\n",
            "Epoch 133, Loss: 0.01596, Val Acc: 0.6829\n",
            "Epoch 134, Loss: 0.02774, Val Acc: 0.6927\n",
            "Epoch 135, Loss: 0.02023, Val Acc: 0.6829\n",
            "Epoch 136, Loss: 0.00943, Val Acc: 0.6878\n",
            "Epoch 137, Loss: 0.01738, Val Acc: 0.6878\n",
            "Epoch 138, Loss: 0.01581, Val Acc: 0.6780\n",
            "Epoch 139, Loss: 0.02640, Val Acc: 0.6683\n",
            "Epoch 140, Loss: 0.02268, Val Acc: 0.6683\n",
            "Epoch 141, Loss: 0.01491, Val Acc: 0.6732\n",
            "Epoch 142, Loss: 0.01340, Val Acc: 0.6780\n",
            "Epoch 143, Loss: 0.00883, Val Acc: 0.6683\n",
            "Epoch 144, Loss: 0.00825, Val Acc: 0.6878\n",
            "Epoch 145, Loss: 0.02134, Val Acc: 0.6878\n",
            "Epoch 146, Loss: 0.01300, Val Acc: 0.6878\n",
            "Epoch 147, Loss: 0.02052, Val Acc: 0.6829\n",
            "Epoch 148, Loss: 0.01853, Val Acc: 0.6829\n",
            "Epoch 149, Loss: 0.00665, Val Acc: 0.6927\n",
            "Epoch 150, Loss: 0.02321, Val Acc: 0.6976\n",
            "Epoch 151, Loss: 0.01658, Val Acc: 0.6976\n",
            "Epoch 152, Loss: 0.01622, Val Acc: 0.7024\n",
            "Epoch 153, Loss: 0.01352, Val Acc: 0.6976\n",
            "Epoch 154, Loss: 0.02886, Val Acc: 0.6976\n",
            "Epoch 155, Loss: 0.02040, Val Acc: 0.7171\n",
            "Epoch 156, Loss: 0.01147, Val Acc: 0.7171\n",
            "Epoch 157, Loss: 0.01403, Val Acc: 0.7122\n",
            "Epoch 158, Loss: 0.01026, Val Acc: 0.7024\n",
            "Epoch 159, Loss: 0.02074, Val Acc: 0.7073\n",
            "Epoch 160, Loss: 0.00957, Val Acc: 0.7122\n",
            "Epoch 161, Loss: 0.00497, Val Acc: 0.7122\n",
            "Epoch 162, Loss: 0.00769, Val Acc: 0.7171\n",
            "Epoch 163, Loss: 0.01305, Val Acc: 0.7171\n",
            "Epoch 164, Loss: 0.01615, Val Acc: 0.7171\n",
            "Epoch 165, Loss: 0.02090, Val Acc: 0.7122\n",
            "Epoch 166, Loss: 0.01271, Val Acc: 0.7024\n",
            "Epoch 167, Loss: 0.01616, Val Acc: 0.7024\n",
            "Epoch 168, Loss: 0.01169, Val Acc: 0.7024\n",
            "Epoch 169, Loss: 0.01129, Val Acc: 0.7073\n",
            "Epoch 170, Loss: 0.01486, Val Acc: 0.7073\n",
            "Epoch 171, Loss: 0.02225, Val Acc: 0.7122\n",
            "Epoch 172, Loss: 0.01079, Val Acc: 0.7220\n",
            "Epoch 173, Loss: 0.03292, Val Acc: 0.7171\n",
            "Epoch 174, Loss: 0.00605, Val Acc: 0.7220\n",
            "Epoch 175, Loss: 0.02991, Val Acc: 0.7220\n",
            "Epoch 176, Loss: 0.01011, Val Acc: 0.7073\n",
            "Epoch 177, Loss: 0.01208, Val Acc: 0.7073\n",
            "Epoch 178, Loss: 0.01519, Val Acc: 0.7073\n",
            "Epoch 179, Loss: 0.03368, Val Acc: 0.7073\n",
            "Epoch 180, Loss: 0.04816, Val Acc: 0.7122\n",
            "Epoch 181, Loss: 0.04263, Val Acc: 0.7171\n",
            "Epoch 182, Loss: 0.01392, Val Acc: 0.7122\n",
            "Epoch 183, Loss: 0.01455, Val Acc: 0.7171\n",
            "Epoch 184, Loss: 0.00927, Val Acc: 0.7171\n",
            "Epoch 185, Loss: 0.00722, Val Acc: 0.7122\n",
            "Epoch 186, Loss: 0.01544, Val Acc: 0.7122\n",
            "Epoch 187, Loss: 0.01272, Val Acc: 0.7073\n",
            "Epoch 188, Loss: 0.01948, Val Acc: 0.7122\n",
            "Epoch 189, Loss: 0.01274, Val Acc: 0.7122\n",
            "Epoch 190, Loss: 0.01850, Val Acc: 0.7073\n",
            "Epoch 191, Loss: 0.00965, Val Acc: 0.7073\n",
            "Epoch 192, Loss: 0.00490, Val Acc: 0.7073\n",
            "Epoch 193, Loss: 0.00976, Val Acc: 0.7171\n",
            "Epoch 194, Loss: 0.00929, Val Acc: 0.7122\n",
            "Epoch 195, Loss: 0.01522, Val Acc: 0.7122\n",
            "Epoch 196, Loss: 0.01044, Val Acc: 0.7171\n",
            "Epoch 197, Loss: 0.02001, Val Acc: 0.7171\n",
            "Epoch 198, Loss: 0.02239, Val Acc: 0.7220\n",
            "Epoch 199, Loss: 0.00946, Val Acc: 0.7171\n",
            "Epoch 200, Loss: 0.01359, Val Acc: 0.7220\n",
            "Epoch 201, Loss: 0.00937, Val Acc: 0.7268\n",
            "Epoch 202, Loss: 0.00729, Val Acc: 0.7220\n",
            "Epoch 203, Loss: 0.01343, Val Acc: 0.7220\n",
            "Epoch 204, Loss: 0.01994, Val Acc: 0.7220\n",
            "Epoch 205, Loss: 0.02767, Val Acc: 0.7171\n",
            "Epoch 206, Loss: 0.02933, Val Acc: 0.7171\n",
            "Epoch 207, Loss: 0.02437, Val Acc: 0.7122\n",
            "Epoch 208, Loss: 0.02067, Val Acc: 0.7073\n",
            "Epoch 209, Loss: 0.01193, Val Acc: 0.7122\n",
            "Epoch 210, Loss: 0.02659, Val Acc: 0.7024\n",
            "Epoch 211, Loss: 0.00840, Val Acc: 0.6976\n",
            "Epoch 212, Loss: 0.05043, Val Acc: 0.6976\n",
            "Epoch 213, Loss: 0.01427, Val Acc: 0.6976\n",
            "Epoch 214, Loss: 0.01256, Val Acc: 0.6976\n",
            "Epoch 215, Loss: 0.02227, Val Acc: 0.6927\n",
            "Epoch 216, Loss: 0.01369, Val Acc: 0.6927\n",
            "Epoch 217, Loss: 0.00733, Val Acc: 0.6878\n",
            "Epoch 218, Loss: 0.01887, Val Acc: 0.6829\n",
            "Epoch 219, Loss: 0.02210, Val Acc: 0.6927\n",
            "Epoch 220, Loss: 0.01001, Val Acc: 0.6927\n",
            "Epoch 221, Loss: 0.01551, Val Acc: 0.6927\n",
            "Epoch 222, Loss: 0.04765, Val Acc: 0.7024\n",
            "Epoch 223, Loss: 0.01743, Val Acc: 0.7073\n",
            "Epoch 224, Loss: 0.01107, Val Acc: 0.7073\n",
            "Epoch 225, Loss: 0.03606, Val Acc: 0.7073\n",
            "Epoch 226, Loss: 0.01431, Val Acc: 0.7024\n",
            "Epoch 227, Loss: 0.02802, Val Acc: 0.7024\n",
            "Epoch 228, Loss: 0.02102, Val Acc: 0.7073\n",
            "Epoch 229, Loss: 0.01454, Val Acc: 0.7073\n",
            "Epoch 230, Loss: 0.01229, Val Acc: 0.7122\n",
            "Epoch 231, Loss: 0.01849, Val Acc: 0.7073\n",
            "Epoch 232, Loss: 0.01321, Val Acc: 0.7171\n",
            "Epoch 233, Loss: 0.00827, Val Acc: 0.7220\n",
            "Epoch 234, Loss: 0.01281, Val Acc: 0.7220\n",
            "Epoch 235, Loss: 0.01016, Val Acc: 0.7122\n",
            "Epoch 236, Loss: 0.01519, Val Acc: 0.7122\n",
            "Epoch 237, Loss: 0.00630, Val Acc: 0.7122\n",
            "Epoch 238, Loss: 0.00824, Val Acc: 0.7024\n",
            "Epoch 239, Loss: 0.01048, Val Acc: 0.7024\n",
            "Epoch 240, Loss: 0.00527, Val Acc: 0.6976\n",
            "Epoch 241, Loss: 0.00850, Val Acc: 0.6976\n",
            "Epoch 242, Loss: 0.01393, Val Acc: 0.6976\n",
            "Epoch 243, Loss: 0.02075, Val Acc: 0.7024\n",
            "Epoch 244, Loss: 0.00995, Val Acc: 0.6976\n",
            "Epoch 245, Loss: 0.00492, Val Acc: 0.7024\n",
            "Epoch 246, Loss: 0.01024, Val Acc: 0.7024\n",
            "Epoch 247, Loss: 0.01015, Val Acc: 0.7024\n",
            "Epoch 248, Loss: 0.01097, Val Acc: 0.7024\n",
            "Epoch 249, Loss: 0.01009, Val Acc: 0.6976\n",
            "Epoch 250, Loss: 0.00462, Val Acc: 0.7024\n",
            "Epoch 251, Loss: 0.00833, Val Acc: 0.7024\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir los datos de prueba a tensores\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
        "\n",
        "gnn_model.eval()\n",
        "\n",
        "# Evaluación\n",
        "with torch.no_grad():\n",
        "    outputs = gnn_model(x_test_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    correct = (predicted == y_test_tensor).sum().item()\n",
        "    total = y_test_tensor.size(0)\n",
        "    test_acc = correct / total\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fyi0X3mim_Yh",
        "outputId": "7908c110-156b-427b-fa15-12d658e5e2b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-54102013a12b>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  node_indices = torch.tensor(node_indices, dtype=torch.long)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RO91P-8L5Rh3"
      },
      "execution_count": 69,
      "outputs": []
    }
  ]
}